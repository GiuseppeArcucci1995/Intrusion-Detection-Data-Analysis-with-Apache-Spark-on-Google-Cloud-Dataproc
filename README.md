# Migrating from Spark to BigQuery via Dataproc â€“ Part 1

This project demonstrates how to modernize a Spark-based analytics pipeline by migrating it to Google Cloud, using Dataproc, Google Cloud Storage (GCS), and eventually BigQuery.  
This notebook represents **Part 1** of the migration process: a simple lift-and-shift of Spark code to Google Cloud Dataproc.

---

## ğŸ“ Project Structure

- `01_spark.ipynb`: Original Spark code running on Dataproc (lift-and-shift)
- `spark_analysis.py`: Python script executed within Dataproc to analyze data and generate results
- `report.png`: Generated visualization of connection types by protocol
- `connections_by_protocol/`: CSV output generated by Spark
- Files are stored in the GCS bucket path: `gs://<your-bucket>/sparktodp/`

---

## ğŸ“Š Dataset Description

- Dataset: **KDD Cup 1999**, 10% sample version
- Format: CSV, preprocessed using Spark RDDs
- Purpose: Simulate a real-world analytics job involving cybersecurity logs

---

## ğŸ” What the Spark Job Does

1. **Reads data** from GCS (`kddcup.data_10_percent.gz`)
2. **Parses** CSV into an RDD with typed schema using `Row()`
3. **Creates a DataFrame** from the RDD
4. Performs:
   - Aggregation by protocol type (`groupBy.count`)
   - Attack statistics via **Spark SQL** queries
5. **Generates a bar chart** using Matplotlib and saves it as `report.png`
6. **Uploads output** (chart + CSV) back to GCS

---

## ğŸš€ How to Run

1. Configure your GCP environment and Dataproc cluster
2. Upload `spark_analysis.py` to your Dataproc cluster or Cloud Storage
3. Submit the job to Dataproc with:
   ```bash
   gcloud dataproc jobs submit pyspark spark_analysis.py --cluster=<your-cluster-name> --region=<region> -- --bucket=<your-bucket>
